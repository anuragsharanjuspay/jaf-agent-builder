# LiteLLM Proxy Configuration for JAF Agent Builder
# 
# To use LiteLLM:
# 1. Install LiteLLM: pip install litellm
# 2. Set your API keys in .env or below
# 3. Run: litellm --config litellm-config.yaml --port 4000
# 4. Set USE_LITELLM=true in your .env

model_list:
  # OpenAI Models
  - model_name: "gpt-4"
    litellm_params:
      model: "gpt-4"
      api_key: ${OPENAI_API_KEY}
  
  - model_name: "gpt-4-turbo"
    litellm_params:
      model: "gpt-4-turbo-preview"
      api_key: ${OPENAI_API_KEY}
  
  - model_name: "gpt-3.5-turbo"
    litellm_params:
      model: "gpt-3.5-turbo"
      api_key: ${OPENAI_API_KEY}
  
  - model_name: "gpt-4o"
    litellm_params:
      model: "gpt-4o"
      api_key: ${OPENAI_API_KEY}
  
  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "gpt-4o-mini"
      api_key: ${OPENAI_API_KEY}
  
  # Anthropic Models
  - model_name: "claude-3-opus"
    litellm_params:
      model: "anthropic/claude-3-opus-20240229"
      api_key: ${ANTHROPIC_API_KEY}
  
  - model_name: "claude-3-sonnet"
    litellm_params:
      model: "anthropic/claude-3-sonnet-20240229"
      api_key: ${ANTHROPIC_API_KEY}
  
  - model_name: "claude-3-haiku"
    litellm_params:
      model: "anthropic/claude-3-haiku-20240307"
      api_key: ${ANTHROPIC_API_KEY}
  
  - model_name: "claude-3.5-sonnet"
    litellm_params:
      model: "anthropic/claude-3-5-sonnet-20240620"
      api_key: ${ANTHROPIC_API_KEY}
  
  # Google Models
  - model_name: "gemini-pro"
    litellm_params:
      model: "gemini/gemini-pro"
      api_key: ${GOOGLE_API_KEY}
  
  - model_name: "gemini-ultra"
    litellm_params:
      model: "gemini/gemini-ultra"
      api_key: ${GOOGLE_API_KEY}
  
  - model_name: "gemini-2.0-flash"
    litellm_params:
      model: "gemini/gemini-2.0-flash-thinking-exp-1219"
      api_key: ${GOOGLE_API_KEY}
  
  # Cohere Models
  - model_name: "command"
    litellm_params:
      model: "cohere/command"
      api_key: ${COHERE_API_KEY}
  
  - model_name: "command-r"
    litellm_params:
      model: "cohere/command-r"
      api_key: ${COHERE_API_KEY}
  
  # Together AI Models (Open Source)
  - model_name: "llama-2-70b"
    litellm_params:
      model: "together_ai/togethercomputer/llama-2-70b-chat"
      api_key: ${TOGETHER_API_KEY}
  
  - model_name: "mixtral-8x7b"
    litellm_params:
      model: "together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1"
      api_key: ${TOGETHER_API_KEY}
  
  - model_name: "codellama-70b"
    litellm_params:
      model: "together_ai/codellama/CodeLlama-70b-Instruct-hf"
      api_key: ${TOGETHER_API_KEY}
  
  # Replicate Models
  - model_name: "llama-2-70b-chat"
    litellm_params:
      model: "replicate/meta/llama-2-70b-chat"
      api_key: ${REPLICATE_API_KEY}
  
  - model_name: "mistral-7b"
    litellm_params:
      model: "replicate/mistralai/mistral-7b-instruct-v0.1"
      api_key: ${REPLICATE_API_KEY}
  
  # Azure OpenAI
  - model_name: "azure-gpt-4"
    litellm_params:
      model: "azure/gpt-4"
      api_base: ${AZURE_API_BASE}
      api_key: ${AZURE_API_KEY}
      api_version: "2024-02-15-preview"
  
  - model_name: "azure-gpt-35-turbo"
    litellm_params:
      model: "azure/gpt-35-turbo"
      api_base: ${AZURE_API_BASE}
      api_key: ${AZURE_API_KEY}
      api_version: "2024-02-15-preview"
  
  # OpenRouter (Access to 100+ models)
  - model_name: "openrouter-auto"
    litellm_params:
      model: "openrouter/auto"
      api_key: ${OPENROUTER_API_KEY}

# General Settings
general_settings:
  master_key: ${LITELLM_MASTER_KEY}  # Optional: for proxy authentication
  database_url: ${DATABASE_URL}      # Optional: for usage tracking
  
# Router Settings
router_settings:
  routing_strategy: "least-cost"     # Options: "least-cost", "latency-based", "simple-shuffle"
  
  # Model aliases for fallback
  model_group_alias:
    "gpt-4": ["gpt-4", "azure-gpt-4", "claude-3-opus"]
    "gpt-3.5-turbo": ["gpt-3.5-turbo", "azure-gpt-35-turbo", "claude-3-haiku"]
    "claude": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"]
    "fast": ["gpt-3.5-turbo", "claude-3-haiku", "gemini-pro"]
    "powerful": ["gpt-4", "claude-3-opus", "gemini-ultra"]
  
  # Fallback models
  fallbacks:
    gpt-4: ["claude-3-opus", "gemini-ultra"]
    claude-3-opus: ["gpt-4", "gemini-ultra"]
    
  # Rate limiting
  rpm: 1000  # Requests per minute
  tpm: 100000  # Tokens per minute
  
# Logging
litellm_settings:
  drop_params: true  # Don't log API keys
  success_callback: ["langfuse"]  # Optional: integrate with observability tools
  failure_callback: ["langfuse"]
  
# Optional: Caching
cache:
  type: "redis"  # or "in-memory"
  ttl: 3600  # Cache for 1 hour

# Optional: Content moderation
content_moderation:
  enabled: false
  provider: "openai"  # Use OpenAI's moderation API